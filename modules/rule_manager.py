from modules.data_asset import DataAsset
from modules.rule_processor import RuleProcessor
from modules.data_model_manager import DataModelManager
from schemas.data_model.dq_rule_schema import DQ_RULE_SCHEMA
from schemas.data_model.brnz_dq_rule_type_schema import BRNZ_DQ_RULE_TYPE_SCHEMA
from pyspark.sql import SparkSession, DataFrame, functions as F
from pyspark.sql.functions import col

import logging

logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(__name__)

spark = SparkSession.builder.appName("DQ-Tool").getOrCreate()

class RuleManager:
    """A class to manage the loading and parsing of rules for data quality validation."""

    def __init__(self, data_source: str):
        self.data_source = data_source

    def upsert_rules(self, df: DataFrame) -> None:
        """Upsert the rules into the gold table.
        :param df: A DataFrame containing the rules.
        """
        data_model_manager = DataModelManager(self.data_source)
        # Remove the columns that are automatically generated by the data model
        cols = set(DQ_RULE_SCHEMA.fieldNames()) - set(["rule_key"])
       
        data_model_manager.merge_in_table(
            df,
            cols,
            primary_key="integration_id",
            destination_table=data_model_manager.DQ_RULE,
        )

    def _process_rules(self, rules_tables: list) -> DataFrame:
        """Process the rules and update the gold table
        :param rules_tables: A list of tables containing the rules.
        """
        updated_df = RuleProcessor(spark).process_rules(rules_tables)
        return updated_df

    def update_rules(self, rules_tables: list) -> None:
        """Process the rules and update the gold tables."""

        print("[*] Updating rules")
        self._update_dim_rule_type()

        if rules_tables:
            print(f"[*] Processing rules from {rules_tables}")
            updated_df = self._process_rules(rules_tables)
        
            print("[*] Upserting rules")
            self.upsert_rules(updated_df)
        else:
            print("[*] No rules to process")

    def _update_dim_rule_type(
        self, dim_rule_type_table: str = "dqops.brnz_sharepoint_dq_rule_types_dq_rule_types"
    ) -> None:
        """Update the dimnension rule types table. Read table from bronze layer and update the one in gold.

        :param ref_rule_types_table: The reference table containing the rule types.
        """
        try:
            df = (
                spark.table(dim_rule_type_table)
                .filter(F.col("current_flag") == True)
                .select(BRNZ_DQ_RULE_TYPE_SCHEMA.fieldNames())
            )
        except Exception as e:
            logger.error(
                f"Error loading DataFrame: {e} \n!!! Will not update the reference table"
            )
            return None

        if df.count() == 0:
            raise ValueError(
                f"No data found in the reference table '{dim_rule_type_table}'"
            )
        else:
            data_model_manager = DataModelManager(self.data_source)
            data_model_manager.merge_in_table(
                df,
                df.columns,
                primary_key="rule_type_key",
                destination_table=data_model_manager.DQ_RULE_TYPE,
            )

    def parse_rules(self, context_manager, rules: DataFrame = None) -> None:
        """Parse rules and apply them to the GXContextManager.

        :param rules: A Pandas DataFrame with the rules.
        :param context_manager: An instance of GXContextManager.
        :return: A dictionary with the parsed rules.
        """

        print("[*] Parsing rules")
        error_rules = []

        if not rules:
            rules = spark.table("dqops.dq_rule").join(
                spark.table("dqops.dq_rule_type").select("rule_type_key", "gx_expectation"),
                on="rule_type_key",
                how="left",
            ).orderBy("rule_key")

        #Debug Print
        print(f"Rules count: {rules.count()}")
        rules.show()

        for row in rules.toLocalIterator():
            if row is None or row['rule_enabled'] == 'N' or row['integration_id'] == 'integration_id':
                print(f"({row['rule_key']}) Rule not enabled, skipping. Rule # {row['rule_no']}")
                continue
            catalog, schema, table_name = (
                row["catalog"],
                row["schema"],
                row["table_name"],
            )
            data_asset_name = f"{catalog}-{schema}-{table_name}"

            # Retrieve or create a DataAsset
            data_asset = context_manager.data_assets.get(data_asset_name, None)
            if data_asset is None:
                context_manager.data_assets[data_asset_name] = DataAsset(catalog, schema, table_name, self.data_source)
                data_asset = context_manager.data_assets[data_asset_name]

            # Update the context with the rule
            context_manager.update_context(row)

            # Create an expectation suite if it doesn't already exist
            suite = context_manager.get_or_create(
                f"s-{data_asset_name}",
                None,
                lambda name: context_manager.context.suites.get(name),
                None,
            )

            # Add the expectation to the DataAsset and suite
            print(f"({row['rule_key']}) Adding expectation: {row['gx_expectation']} - Rule type key ({row['rule_type_key']}) - Rule # {row['rule_no']}")
            expectation, enabled = data_asset.get_expectation(row)

            # Add the expectation to the suite if it is enabled
            if enabled and expectation:
                suite.add_expectation(expectation.gx_expectation)
                # We add the expectation after the suite to ensure it has an ID
                data_asset.add_expectation(expectation)
            else:
                print(f"\t- ({row['rule_key']}) Expectation not added: {row['gx_expectation']} - Rule type key ({row['rule_type_key']})")
                error_rules.append(row['rule_key'])

        print(f"Error rules: {error_rules}")